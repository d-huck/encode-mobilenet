{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodec import EncodecModel \n",
    "from encodec.quantization import ResidualVectorQuantizer\n",
    "from encodec.utils import convert_audio\n",
    "from mobilenetv3.mobilenetv3 import hswish, hsigmoid, SeModule, Block\n",
    "\n",
    "import torchaudio\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import init\n",
    "\n",
    "import IPython\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data\n",
    "\n",
    "GTZAN is a good classification dataset for development. It consists of audio/text pairs of music/genre and is a fairly easy task to hit mid-high 90s on given MFCCs or waveforms. This section sets up the dataset. The only reason to run it is if you need to generate encodings at a higher bitrate for further development. The 1.5 bitrate target is already prepared in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhuck/.pyenv/versions/3.8.11/envs/thesis/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "gtzan = load_dataset(\"marsyas/gtzan\")\n",
    "encoder = EncodecModel.encodec_model_24khz()\n",
    "\n",
    "def pre_process_gtzan(gtzan: DatasetDict, target_sr, target_channels) -> (list, list):\n",
    "    \"\"\"\n",
    "    Pre-load the data and process it to the correct sample rate and mono/stereo.\n",
    "    Returns the pre processed data and a list of the targets.\n",
    "    \"\"\"\n",
    "    data, targets = [], []\n",
    "    for x in tqdm(gtzan['train']):\n",
    "        audio, sr = torchaudio.load(x['file'])\n",
    "        audio = convert_audio(audio, sr, target_sr, target_channels)\n",
    "        audio = audio.narrow(-1, 0, target_sr * 10)  # limit to 10 seconds\n",
    "        data.append(audio.unsqueeze(0))\n",
    "        targets.append(x['genre'])\n",
    "        \n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:17<00:00, 57.46it/s]\n"
     ]
    }
   ],
   "source": [
    "data, targets = pre_process_gtzan(gtzan, encoder.sample_rate, encoder.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhuck/.pyenv/versions/3.8.11/envs/thesis/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:34<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "def encode_data(data, encoder, batch_size=8, device=None):\n",
    "    print(\"Pre-encoding training data\")\n",
    "    \n",
    "    encodings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(data), batch_size)):\n",
    "            batch = torch.cat(data[i:i+batch_size], dim=0).to(device)\n",
    "            encoded_frames = encoder.encode(batch)\n",
    "            \n",
    "            codes = torch.cat([e[0] for e in encoded_frames], dim=-1)\n",
    "            encodings.append(codes)\n",
    "    \n",
    "    encodings = torch.cat(encodings, dim=0)\n",
    "    return encodings\n",
    "\n",
    "# This takes about 5 minutes to run on a M1 Macbook Pro, a couple of minutes for a GPU\n",
    "encoder = EncodecModel.encodec_model_24khz()\n",
    "encoder.set_target_bandwidth(3.0)\n",
    "encodings = encode_data(data, encoder, batch_size=8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 750])\n"
     ]
    }
   ],
   "source": [
    "print(encodings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'data': encodings,\n",
    "    'targets': targets\n",
    "}\n",
    "torch.save(dataset, \"gtzan_encodings-3.0.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, data, labels, device=None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "        self.labels = [torch.tensor(x) for x in labels]\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "            data = self.data[index].to(self.device)\n",
    "            target = self.labels[index].to(self.device)\n",
    "            \n",
    "            return data, target\n",
    "\n",
    "def split_data(data, batch_size=32, random_seed=42, device=None, valid_size=0.1, test_size=0.05, shuffle=True):\n",
    "    x = data['data']\n",
    "    y = data['targets']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_seed)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=valid_size, random_state=random_seed)\n",
    "\n",
    "    train = GTZANDataset(x_train, y_train, device=device)\n",
    "    valid = GTZANDataset(x_valid, y_valid, device=device)\n",
    "    test = GTZANDataset(x_test, y_test, device=device)\n",
    "    \n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "data = torch.load(\"./gtzan_encodings-3.0.data\")\n",
    "train, valid, test = split_data(data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Example\n",
    "\n",
    "The stored representations are indices into the quantized code book, so we must recover the codes before we can train on them. This following snippet is an example on how to do that. We could probably make this a little cleaner by not initializing the entire encodec model, but this is the simplest way to do so. After initialization, we can send just the quantizer to device to retrieve the codes as part of the `forward` call of any network. This allows the storage of a dataset on device to remain rather small.\n",
    "\n",
    "One small caveat is that the quantizer expects a shape of `(n_residuals, batch_size, frames)`, so we need to transpose the input to get the right output from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 750]) torch.Size([32, 128, 750])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhuck/.pyenv/versions/3.8.11/envs/thesis/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "encoder = EncodecModel.encodec_model_24khz()\n",
    "encoder.set_target_bandwidth(1.5)\n",
    "quantizer = encoder.quantizer\n",
    "data, targets = next(iter(train))\n",
    "data = data.transpose(0, 1)\n",
    "quantized = quantizer.decode(data)\n",
    "print(data.shape, quantized.shape)\n",
    "quantized = quantized.unsqueeze(1) # add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "The next few blocks are tinkering with model size and individual Block size to make them work with out data. Since we are targeting 10s of audio, our input to a block of the mobilenet should be `(batch_size, 128, n_frames)` The input of the standard MobileNetV3 is `(batch_size, 224, 224, 3)`. First thought is to just use a learnable projection to put it into the right dimensionality expected by the base model and ignore the three channels on the first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "proj = nn.ConvTranspose2d(1, 3, kernel_size=(2,3), stride=(2,1), padding=(16, 264), bias=False)\n",
    "projected = proj(quantized)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we should be able to use the MobileNet as is. We'll add the quantizer as the first step of the forward pass and ensure to freeze it so we don't end up back propping to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhuck/.pyenv/versions/3.8.11/envs/thesis/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first layer of mobilenet\n",
    "class MobileNetV3_Smol(nn.Module):\n",
    "    def __init__(self, encodec_bw=1.5, num_classes=10, act=nn.Hardswish):\n",
    "        super(MobileNetV3_Smol, self).__init__()\n",
    "        encoder = EncodecModel.encodec_model_24khz()\n",
    "        encoder.set_target_bandwidth(encodec_bw)\n",
    "        self.quantizer = encoder.quantizer\n",
    "        self.quantizer.requires_grad_(False)\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 3, kernel_size=(2, 3), stride=(2, 1), padding=(16, 264), bias=False),\n",
    "            nn.BatchNorm2d(3),\n",
    "            act(inplace=True)\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.hs1=act(inplace=True)\n",
    "        \n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 16, 16, 16, nn.ReLU, True, 2),\n",
    "            Block(3, 16, 72, 24, nn.ReLU, False, 2),\n",
    "            Block(3, 24, 88, 24, nn.ReLU, False, 1),\n",
    "            Block(5, 24, 96, 40, act, True, 2),\n",
    "            Block(5, 40, 240, 40, act, True, 1),\n",
    "            Block(5, 40, 240, 40, act, True, 1),\n",
    "            Block(5, 40, 120, 48, act, True, 1),\n",
    "            Block(5, 48, 144, 48, act, True, 1),\n",
    "            Block(5, 48, 288, 96, act, True, 2),\n",
    "            Block(5, 96, 576, 96, act, True, 1),\n",
    "            Block(5, 96, 576, 96, act, True, 1),\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(576)\n",
    "        self.hs2 = act(inplace=True)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.linear3 = nn.Linear(576, 1280, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = act(inplace=True)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # decode from the encodec representation\n",
    "        x = x.transpose(0, 1) \n",
    "        x = self.quantizer.decode(x)\n",
    "        \n",
    "        x = x.unsqueeze(1) # add in a channel dimension\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # run mobile net projection\n",
    "        x = self.hs1(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        #run the bnet\n",
    "        x = self.bneck(x)\n",
    "        \n",
    "        # classify\n",
    "        x = self.hs2(self.bn2(self.conv2(x)))\n",
    "        x = self.gap(x).flatten(1)\n",
    "        x = self.drop(self.hs3(self.bn3(self.linear3(x))))\n",
    "        \n",
    "        return self.linear4(x)\n",
    "        \n",
    "model = MobileNetV3_Smol()\n",
    "\n",
    "x, y = next(iter(train))\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [02:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dhuck/cs.utexas.edu/homework/final/Training.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dhuck/cs.utexas.edu/homework/final/Training.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m out \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dhuck/cs.utexas.edu/homework/final/Training.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dhuck/cs.utexas.edu/homework/final/Training.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dhuck/cs.utexas.edu/homework/final/Training.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dhuck/cs.utexas.edu/homework/final/Training.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m training_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.11/envs/thesis/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.11/envs/thesis/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 25\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for x, y in train:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        total, correct = 0, 0\n",
    "        for x, y in valid:\n",
    "            out = model(x)\n",
    "            _, pred = torch.max(out, dim=1)\n",
    "            total += len(y)\n",
    "            correct += torch.sum(pred == y)\n",
    "        \n",
    "        valid_loss += criterion(out, y).item()\n",
    "        \n",
    "        print(f\"Epoch {i}: training_loss (total) : {training_loss} | valid_loss: {valid_loss} | accuracy: {correct / total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
