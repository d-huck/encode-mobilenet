{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodec import EncodecModel \n",
    "from encodec.quantization import ResidualVectorQuantizer\n",
    "from encodec.utils import convert_audio\n",
    "from mobilenetv3.mobilenetv3 import hswish, hsigmoid, SeModule, Block\n",
    "\n",
    "import torchaudio\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import init\n",
    "\n",
    "import IPython\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data\n",
    "\n",
    "GTZAN is a good classification dataset for development. It consists of audio/text pairs of music/genre and is a fairly easy task to hit mid-high 90s on given MFCCs or waveforms. This section sets up the dataset. The only reason to run it is if you need to generate encodings at a higher bitrate for further development. The 1.5 bitrate target is already prepared in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtzan = load_dataset(\"marsyas/gtzan\")\n",
    "encoder = EncodecModel.encodec_model_24khz()\n",
    "\n",
    "def pre_process_gtzan(gtzan: DatasetDict, target_sr, target_channels) -> (list, list):\n",
    "    \"\"\"\n",
    "    Pre-load the data and process it to the correct sample rate and mono/stereo.\n",
    "    Returns the pre processed data and a list of the targets.\n",
    "    \"\"\"\n",
    "    data, targets = [], []\n",
    "    for x in tqdm(gtzan['train']):\n",
    "        audio, sr = torchaudio.load(x['file'])\n",
    "        audio = convert_audio(audio, sr, target_sr, target_channels)\n",
    "        audio = audio.narrow(-1, 0, target_sr * 10)  # limit to 10 seconds\n",
    "        data.append(audio.unsqueeze(0))\n",
    "        targets.append(x['genre'])\n",
    "        \n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = pre_process_gtzan(gtzan, encoder.sample_rate, encoder.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data, encoder, batch_size=8, device=None):\n",
    "    print(\"Pre-encoding training data\")\n",
    "    \n",
    "    encodings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(data), batch_size)):\n",
    "            batch = torch.cat(data[i:i+batch_size], dim=0).to(device)\n",
    "            encoded_frames = encoder.encode(batch)\n",
    "            \n",
    "            codes = torch.cat([e[0] for e in encoded_frames], dim=-1)\n",
    "            encodings.append(codes)\n",
    "    \n",
    "    encodings = torch.cat(encodings, dim=0)\n",
    "    return encodings\n",
    "\n",
    "# This takes about 5 minutes to run on a M1 Macbook Pro, a couple of minutes for a GPU\n",
    "encoder = EncodecModel.encodec_model_24khz()\n",
    "encoder.set_target_bandwidth(3.0)\n",
    "encodings = encode_data(data, encoder, batch_size=8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encodings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'data': encodings,\n",
    "    'targets': targets\n",
    "}\n",
    "torch.save(dataset, \"gtzan_encodings-3.0.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, data, labels, device=None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "        self.labels = [torch.tensor(x) for x in labels]\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "            data = self.data[index].to(self.device)\n",
    "            target = self.labels[index].to(self.device)\n",
    "            \n",
    "            return data, target\n",
    "\n",
    "def split_data(data, batch_size=32, random_seed=42, device=None, valid_size=0.1, test_size=0.05, shuffle=True):\n",
    "    x = data['data']\n",
    "    y = data['targets']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_seed)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=valid_size, random_state=random_seed)\n",
    "\n",
    "    train = GTZANDataset(x_train, y_train, device=device)\n",
    "    valid = GTZANDataset(x_valid, y_valid, device=device)\n",
    "    test = GTZANDataset(x_test, y_test, device=device)\n",
    "    \n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "data = torch.load(\"./gtzan_encodings-3.0.data\")\n",
    "train, valid, test = split_data(data, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Example\n",
    "\n",
    "The stored representations are indices into the quantized code book, so we must recover the codes before we can train on them. This following snippet is an example on how to do that. We could probably make this a little cleaner by not initializing the entire encodec model, but this is the simplest way to do so. After initialization, we can send just the quantizer to device to retrieve the codes as part of the `forward` call of any network. This allows the storage of a dataset on device to remain rather small.\n",
    "\n",
    "One small caveat is that the quantizer expects a shape of `(n_residuals, batch_size, frames)`, so we need to transpose the input to get the right output from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncodecModel.encodec_model_24khz()\n",
    "encoder.set_target_bandwidth(1.5)\n",
    "quantizer = encoder.quantizer\n",
    "data, targets = next(iter(train))\n",
    "data = data.transpose(0, 1)\n",
    "quantized = quantizer.decode(data)\n",
    "print(data.shape, quantized.shape)\n",
    "quantized = quantized.unsqueeze(1) # add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "The next few blocks are tinkering with model size and individual Block size to make them work with out data. Since we are targeting 10s of audio, our input to a block of the mobilenet should be `(batch_size, 128, n_frames)` The input of the standard MobileNetV3 is `(batch_size, 224, 224, 3)`. First thought is to just use a learnable projection to put it into the right dimensionality expected by the base model and ignore the three channels on the first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = nn.ConvTranspose2d(1, 3, kernel_size=(2,3), stride=(2,1), padding=(16, 264), bias=False)\n",
    "projected = proj(quantized)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we should be able to use the MobileNet as is. We'll add the quantizer as the first step of the forward pass and ensure to freeze it so we don't end up back propping to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.randn(1, 1, 128, 750)\n",
    "projection = nn.Sequential(\n",
    "    nn.Linear(750, 128),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "conv1 = nn.Conv2d(1, 3, kernel_size=21, stride=1, padding=2, bias=False)\n",
    "\n",
    "out = conv1(projection(rand))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand2 = torch.randn(1, 3, 224, 224)\n",
    "conv2 = nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "out = conv2(rand2)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer of mobilenet\n",
    "class MobileNetV3_Smol(nn.Module):\n",
    "    def __init__(self, encodec_bw=1.5, num_classes=10, act=nn.Hardswish):\n",
    "        super(MobileNetV3_Smol, self).__init__()\n",
    "        encoder = EncodecModel.encodec_model_24khz()\n",
    "        encoder.set_target_bandwidth(encodec_bw)\n",
    "        self.quantizer = encoder.quantizer\n",
    "        self.quantizer.requires_grad = False\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 3, kernel_size=(2, 3), stride=(2, 1), padding=(16, 264), bias=False),\n",
    "            nn.BatchNorm2d(3),\n",
    "            act(inplace=True)\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.hs1=act(inplace=True)\n",
    "        \n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 16, 16, 16, nn.ReLU, True, 2),\n",
    "            Block(3, 16, 72, 24, nn.ReLU, False, 2),\n",
    "            Block(3, 24, 88, 24, nn.ReLU, False, 1),\n",
    "            Block(5, 24, 96, 40, act, True, 2),\n",
    "            Block(5, 40, 240, 40, act, True, 1),\n",
    "            Block(5, 40, 240, 40, act, True, 1),\n",
    "            Block(5, 40, 120, 48, act, True, 1),\n",
    "            Block(5, 48, 144, 48, act, True, 1),\n",
    "            Block(5, 48, 288, 96, act, True, 2),\n",
    "            Block(5, 96, 576, 96, act, True, 1),\n",
    "            Block(5, 96, 576, 96, act, True, 1),\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(576)\n",
    "        self.hs2 = act(inplace=True)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.linear3 = nn.Linear(576, 1280, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = act(inplace=True)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # decode from the encodec representation\n",
    "        x = x.transpose(0, 1) \n",
    "        x = self.quantizer.decode(x)\n",
    "        \n",
    "        x = x.unsqueeze(1) # add in a channel dimension\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # run mobile net projection\n",
    "        x = self.hs1(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        #run the bnet\n",
    "        x = self.bneck(x)\n",
    "        \n",
    "        # classify\n",
    "        x = self.hs2(self.bn2(self.conv2(x)))\n",
    "        x = self.gap(x).flatten(1)\n",
    "        x = self.drop(self.hs3(self.bn3(self.linear3(x))))\n",
    "        \n",
    "        return self.linear4(x)\n",
    "        \n",
    "model = MobileNetV3_Smol()\n",
    "\n",
    "x, y = next(iter(train))\n",
    "out = model(x.to('cpu'))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer of mobilenet\n",
    "class MobileNetV3_LARGE(nn.Module):\n",
    "    def __init__(self, encodec_bw=1.5, num_classes=10, act=nn.Hardswish):\n",
    "        super(MobileNetV3_LARGE, self).__init__()\n",
    "        encoder = EncodecModel.encodec_model_24khz()\n",
    "        encoder.set_target_bandwidth(encodec_bw)\n",
    "        self.quantizer = encoder.quantizer\n",
    "        self.quantizer.requires_grad = False\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(750, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=21, stride=1, padding=2, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.hs1 = act(inplace=True)\n",
    "        \n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 16, 16, 16, nn.ReLU, False, 1),\n",
    "            Block(3, 16, 64, 24, nn.ReLU, False, 2),\n",
    "            Block(3, 24, 72, 24, nn.ReLU, False, 1),\n",
    "            Block(5, 24, 72, 40, nn.ReLU, True, 2),\n",
    "            Block(5, 40, 120, 40, nn.ReLU, True, 1),\n",
    "            Block(5, 40, 120, 40, nn.ReLU, True, 1),\n",
    "            Block(3, 40, 240, 80, act, False, 2),\n",
    "            Block(3, 80, 200, 80, act, False, 1),\n",
    "            Block(3, 80, 184, 80, act, False, 1),\n",
    "            Block(3, 80, 184, 80, act, False, 1),\n",
    "            Block(3, 80, 480, 112, act, True, 1),\n",
    "            Block(3, 112, 672, 112, act, True, 1),\n",
    "            Block(5, 112, 672, 160, act, True, 2),\n",
    "            Block(5, 160, 672, 160, act, True, 1),\n",
    "            Block(5, 160, 960, 160, act, True, 1),\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(160, 960, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(960)\n",
    "        self.hs2 = act(inplace=True)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.linear3 = nn.Linear(960, 1280, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = act(inplace=True)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # decode from the encodec representation\n",
    "        x = x.transpose(0, 1) \n",
    "        x = self.quantizer.decode(x)\n",
    "        \n",
    "        x = x.unsqueeze(1) # add in a channel dimension\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # run mobile net projection\n",
    "        x = self.hs1(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        #run the bnet\n",
    "        x = self.bneck(x)\n",
    "        \n",
    "        # classify\n",
    "        x = self.hs2(self.bn2(self.conv2(x)))\n",
    "        x = self.gap(x).flatten(1)\n",
    "        x = self.drop(self.hs3(self.bn3(self.linear3(x))))\n",
    "        \n",
    "        return self.linear4(x)\n",
    "        \n",
    "model = MobileNetV3_LARGE()\n",
    "\n",
    "x, y = next(iter(train))\n",
    "print(y)\n",
    "out = model(x.to('cpu'))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV3_LARGE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "n_epochs = 250\n",
    "for i in range(n_epochs):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    for x, y in train:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        total, correct = 0, 0\n",
    "        for x, y in valid:\n",
    "            out = model(x)\n",
    "            _, pred = torch.max(out, dim=1)\n",
    "            total += len(y)\n",
    "            correct += torch.sum(pred == y)\n",
    "        \n",
    "            valid_loss += criterion(out, y).item()\n",
    "        \n",
    "        print(f\"Epoch {i}: training_loss (total) : {training_loss} | valid_loss: {valid_loss} | accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtzan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "# dataset = dataset.sort(\"id\")\n",
    "dataset = load_dataset(\"marsyas/gtzan\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "predicted_label = model.config.id2label[predicted_class_ids]\n",
    "predicted_label\n",
    "\n",
    "# compute loss - target_label is e.g. \"down\"\n",
    "target_label = model.config.id2label[0]\n",
    "inputs[\"labels\"] = torch.tensor([model.config.label2id[target_label]])\n",
    "loss = model(**inputs).loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    train_dataset=gtzan,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"AST Finetune\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"initial\" # log all model checkpoints\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hear21passt.base import get_basic_model, get_model_passt\n",
    "\n",
    "model = get_basic_model(mode=\"logits\")\n",
    "model.net = get_model_passt(arch=\"passt_s_swa_p16_128_ap476\",  n_classes=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
