{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodec import EncodecModel \n",
    "from encodec.quantization import ResidualVectorQuantizer\n",
    "from encodec.utils import convert_audio\n",
    "from mobilenetv3.mobilenetv3 import hswish, hsigmoid, SeModule, Block\n",
    "\n",
    "import torchaudio\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import IPython\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data\n",
    "\n",
    "GTZAN is a good classification dataset for development. It consists of audio/text pairs of music/genre and is a fairly easy task to hit mid-high 90s on given MFCCs or waveforms. This section sets up the dataset. The only reason to run it is if you need to generate encodings at a higher bitrate for further development. The 1.5 bitrate target is already prepared in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhuck/.pyenv/versions/3.8.11/envs/thesis/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "gtzan = load_dataset(\"marsyas/gtzan\")\n",
    "encoder = EncodecModel.encodec_model_24khz()\n",
    "\n",
    "def pre_process_gtzan(gtzan: DatasetDict, target_sr, target_channels) -> (list, list):\n",
    "    \"\"\"\n",
    "    Pre-load the data and process it to the correct sample rate and mono/stereo.\n",
    "    Returns the pre processed data and a list of the targets.\n",
    "    \"\"\"\n",
    "    data, targets = [], []\n",
    "    for x in tqdm(gtzan['train']):\n",
    "        audio, sr = torchaudio.load(x['file'])\n",
    "        audio = convert_audio(audio, sr, target_sr, target_channels)\n",
    "        audio = audio.narrow(-1, 0, target_sr * 10)  # limit to 10 seconds\n",
    "        data.append(audio.unsqueeze(0))\n",
    "        targets.append(x['genre'])\n",
    "        \n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:17<00:00, 55.60it/s]\n"
     ]
    }
   ],
   "source": [
    "data, targets = pre_process_gtzan(gtzan, encoder.sample_rate, encoder.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhuck/.pyenv/versions/3.8.11/envs/thesis/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:40<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "def encode_data(data, encoder, batch_size=8, device=None):\n",
    "    print(\"Pre-encoding training data\")\n",
    "    \n",
    "    encodings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(data), batch_size)):\n",
    "            batch = torch.cat(data[i:i+batch_size], dim=0).to(device)\n",
    "            encoded_frames = encoder.encode(batch)\n",
    "            \n",
    "            codes = torch.cat([e[0] for e in encoded_frames], dim=-1)\n",
    "            encodings.append(codes)\n",
    "    \n",
    "    encodings = torch.cat(encodings, dim=0)\n",
    "    return encodings\n",
    "\n",
    "# This takes about 5 minutes to run on a M1 Macbook Pro, a couple of minutes for a GPU\n",
    "encoder = EncodecModel.encodec_model_24khz()\n",
    "encoder.set_target_bandwidth(1.5)\n",
    "encodings = encode_data(data, encoder, batch_size=8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 750])\n"
     ]
    }
   ],
   "source": [
    "print(encodings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'data': encodings,\n",
    "    'targets': targets\n",
    "}\n",
    "torch.save(dataset, \"gtzan_encodings-1.5.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, data, labels, device=None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "        self.labels = [torch.tensor(x) for x in labels]\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "            data = self.data[index].to(self.device)\n",
    "            target = self.labels[index].to(self.device)\n",
    "            \n",
    "            return data, target\n",
    "\n",
    "def split_data(data, batch_size=32, random_seed=42, device=None, valid_size=0.1, test_size=0.05, shuffle=True):\n",
    "    x = data['data']\n",
    "    y = data['targets']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_seed)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=valid_size, random_state=random_seed)\n",
    "\n",
    "    train = GTZANDataset(x_train, y_train, device=device)\n",
    "    valid = GTZANDataset(x_valid, y_valid, device=device)\n",
    "    test = GTZANDataset(x_test, y_test, device=device)\n",
    "    \n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "data = torch.load(\"./gtzan_encodings-1.5.data\")\n",
    "train, valid, test = split_data(data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Example\n",
    "\n",
    "The stored representations are indices into the quantized code book, so we must recover the codes before we can train on them. This following snippet is an example on how to do that. We could probably make this a little cleaner by not initializing the entire encodec model, but this is the simplest way to do so. After initialization, we can send just the quantizer to device to retrieve the codes as part of the `forward` call of any network. This allows the storage of a dataset on device to remain rather small.\n",
    "\n",
    "One small caveat is that the quantizer expects a shape of `(n_residuals, batch_size, frames)`, so we need to transpose the input to get the right output from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 750]) torch.Size([32, 128, 750])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhuck/.pyenv/versions/3.8.11/envs/thesis/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "encoder = EncodecModel.encodec_model_24khz()\n",
    "encoder.set_target_bandwidth(1.5)\n",
    "quantizer = encoder.quantizer\n",
    "data, targets = next(iter(train))\n",
    "data = data.transpose(0, 1)\n",
    "quantized = quantizer.decode(data)\n",
    "print(data.shape, quantized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "The next few blocks are tinkering with model size and individual Block size to make them work with out data. Since we are targeting 10s of audio, our input to a block of the mobilenet should be `(batch_size, 128, n_frames)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
